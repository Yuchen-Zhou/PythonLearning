{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn教程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果你还没有安装sklearn可以运行这行代码\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.监督学习Supervised Learning\n",
    "### 1.1 广义线性模型 Linear Model\n",
    "主要讲述一些用于回归的方法，其中目标值 y 是输入变量 x 的线性组合。 数学概念表示为：如果 $\\hat{y}$ 是预测值，那么有：\n",
    "$$\n",
    "\\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p\n",
    "$$\n",
    "\n",
    "在整个模块中，我们定义向量 w = (w_1,..., w_p) 作为 coef_ ，定义 w_0 作为 intercept_ \n",
    "\n",
    "\n",
    "#### 1.1.1 普通最小二乘法 Ordinary Least Squares\n",
    "`LinearRegression`拟合一个带有系数$w = (w_1, ..., w_p) $的线性模型，使数据集实际观测数据和预测数据（估计值）之间的残差平方和最小。其数学表达式为:\n",
    "$$\n",
    "\\underset{w}{min\\,} {|| X w - y||_2}^2\n",
    "$$\n",
    "\n",
    "<img src='../pics/sphx_glr_plot_ols_001.png' width='50%'>\n",
    "\n",
    "LinearRegression 会调用`fit`方法来拟合数组 X， y，并且将线性模型的系数 `w`存储在其成员变量 coef_ 中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，对于普通最小二乘的系数估计问题，其依赖于模型各项的相互独立性。当各项是相关的，且设计矩阵 `X` 的各列近似线性相关，那么，设计矩阵会趋向于奇异矩阵，这种特性导致最小二乘估计对于随机误差非常敏感，可能产生很大的方差。例如，在没有实验设计的情况下收集到的数据，这种多重共线性（multicollinearity）的情况可能真的会出现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**普通最小二乘法的复杂度**  \n",
    "该方法使用X的奇异值分解来计算最小二乘解。如果X是一个形状为`(x_samples, n_features)`的矩阵，设$n_{sample}\\geq_n{feature}$, 则该方法的复杂度为$O(n_{samples}n_{smaples}^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 岭回归\n",
    "Rige回归通过对系数的大小施加惩罚来解决普通最小二乘法的一些问题。岭系数最小化的是带惩罚项的残差平方和，\n",
    "$$\n",
    "\\underset{w}{min\\,} {{|| X w - y||_2}^2 + \\alpha {||w||_2}^2}\n",
    "$$\n",
    "\n",
    "其中，`α≥0`是控制系数收缩量的复杂性参数：α的值越大，收缩量越大，模型对共线性的鲁棒性也更强 \n",
    "<img src='../pics/reg-2.jpeg' wifth='50%'>\n",
    "\n",
    "与其他线性模型一样，Ridge用`fit()`方法完成拟合，并将模型系数`ω`存储在其`coef_`成员中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34545455 0.34545455]\n",
      "0.1363636363636364\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "reg = linear_model.Ridge(alpha= .5)\n",
    "x = np.array([[0, 0], [0, 0], [1, 1]])\n",
    "y = np.array([0, .1, 1])\n",
    "reg.fit(x, y)\n",
    "\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**岭回归的复杂度**\n",
    "与普通最小二乘法的复杂度是相同的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**设置正则化参数：广义交叉验证**  \n",
    "`RidgeCV`通过内置的关于的alpha参数的交叉验证来实现岭回归。该对象与GridSearchCV的使用方式相同，只是它默认为Generalized Cross-Validation（广义交叉验证GCV），这是一种有效的留一验证方法（LOO-CV）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "reg.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定cv属性的值将触发（通过GridSearchCV的）的交叉验证。例如，cv=10将出发10折的交叉验证，而不是广义交叉验证（GCV）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Lasso\n",
    "`Lasso`是拟合稀疏系统的线性模型。它在一些情况下是有用的，因为它倾向于使用较少参数值的情况，有效地减少给定解决方案所依赖变量的数量。因此，Lasso及其变体是压缩感知领域的基础。子啊一定条件下，它可以恢复一组非零权重的精确集。  \n",
    "在数学公式表达上，它由一个带有` \\ell_1 `先验的正则项的线性模型组成。其最小化的目标函数是：\n",
    "$$\n",
    "\\underset{w}{min\\,} { \\frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}\n",
    "$$\n",
    "lasso estimate解决了加上罚项$ \\alpha ||w||_1$的最小二乘法的最小化值，其中，α是一个常数，`||w||_1`是参数向量的`\\ell_1`-norm范数   \n",
    "\n",
    "`Lasso`类的实现使用了coordinate descent（坐标下降算法）来拟合系数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha= 0.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "reg.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**设置正则化参数**  \n",
    "`alpha`参数控制估计系数的稀疏度\n",
    "\n",
    "- 使用交叉验证  \n",
    "scikit-learn通过交叉验证来公开设置Lasso`alpha`参数的对象：`LassoCV`和`LassoLarsCV`。`LassoLarsCV`是基于下面将要提到的最小角回归算法。对于具有许多线性回归的高纬数据集，`LassoCV`最常见。然而，`LassoLarsCV`在寻找`alpha`参数值上更具有优势,而且如果样本数量比特征数量少得多时，通常`LassoLarsCV`比`LassoCV`要快。\n",
    "\n",
    "<img src='../pics/reg-3.png' width='50%'>\n",
    "\n",
    "<img src='../pics/reg-4.png' width='50%'>\n",
    "\n",
    "- 基于信息标准的模型选择\n",
    "有多种选择时，估计器`LassoLarsIC`建议使用Akaike information criterion(Akaike信息判断)（AIC）或Bayes Information criterion（贝叶斯信息判据）（BIC）。当使用f-fold交叉验证时，正则化路径只计算一次而不是k+1次，所以找到α的最优解值是一种计算上更经济的代替方法。然而，这种的判据需要对解决方案的自由度进行适当的估计，它会假设模型是正确的，对大样本（渐进结果）进行导出，即数据实际上是由该模型生成的。当问题严重受限（比样本更多的特征）时，它们也容易崩溃。\n",
    "<img src='../pics/reg-5.png' width='50%'>\n",
    "\n",
    "- 与SVM的正则化参数的比较\n",
    "`alpha`和SVM的正则化参数`C`之间的等式关系是`alpha = 1 / C`或`alpha = 1 / (n_samples * C)`，并依赖于估计器和模型优化的确切的目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4. 多任务Lasso\n",
    "`MultiTaskLasso`是一个估计多元回归系数系数的线性模型：`y`是一个形状为`(n_samples, n_tasks)`的二维数组，其约束条件和其他回归问题（也称为任务）是一样的，都是所选的特征值。  \n",
    "下图比较了通过简单的Lasso或MultiTaskLasso得到的W中非零的位置。Lasso估计产生分散的非零值，而MultiTaskLasso的一整列都是非零的。 \n",
    "\n",
    "<img src='../pics/reg-6.png' width='50%'>\n",
    "<img src='../pics/reg-7.png' width='50%'>\n",
    "\n",
    "在数学上，它由一个线性模型组成，以混合的 \\ell_1 \\ell_2 作为正则化器进行训练。目标函数最小化是：\n",
    "$$\n",
    "\\underset{w}{min\\,} { \\frac{1}{2n_{samples}} ||X W - Y||_{Fro} ^ 2 + \\alpha ||W||_{21}}\n",
    "$$\n",
    "其中 Fro 表示 Frobenius 标准：\n",
    "$$\n",
    "||A||_{Fro} = \\sqrt{\\sum_{ij} a_{ij}^2}\n",
    "$$\n",
    "并且 $\\ell_1 \\ell_2$ 读取为:\n",
    "$$\n",
    "||A||_{2 1} = \\sum_i \\sqrt{\\sum_j a_{ij}^2}\n",
    "$$\n",
    "`MultiTaskLasso` 类的实现使用了坐标下降作为拟合系数的算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5.弹性网络\n",
    "`弹性网络`是一种使用L1，L2范数作为先验正则项训练的线性回归模型。这种组合允许拟合到一个只有少量参数是非零稀疏的模型，就像`Lasso`一样，但是它仍然保持了一些类似于`Ridge`的正则性质。我们可利用`l1_ratio`参数控制L1和L2的凸组合。\n",
    "\n",
    "弹性网络在很多特征互相联系的情况下是非常有用的。Lasso很可能只随机考虑这些特征中的一个，而弹性网络更倾向于选择两个。  \n",
    "\n",
    "在实践中，Lasso和Ridge之间权衡的一个优势是它允许在循环过程中(Under rotate)中继承Ridge的稳定性。  \n",
    "\n",
    "在这里，最小化的目标函数是\n",
    "\n",
    "$$\n",
    "\\underset{w}{min\\,} { \\frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \\alpha \\rho ||w||_1 +\\frac{\\alpha(1-\\rho)}{2} ||w||_2 ^ 2}\n",
    "$$\n",
    "`ElasticNetCV`类可以通过交叉验证来设置参数`alpha`(α)和`l1_ratio`(ρ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6.多任务弹性网络\n",
    "`MultiTaskElasticNet`是一个对多回归问题估算稀疏参数的弹性网络：`Y`是一个二维数组，形状是`(n_samples, n_tasks)`。其限制条件是和其他回归问题一样，是选择的特征，也称为tasks。  \n",
    "\n",
    "从数学上来说，它包含一个混合的$\\ell_1 \\ell_2$先验和$\\ell_2$先验为正则项训练的线性模型目标函数就是最小化：\n",
    "$$\n",
    "\\underset{w}{min\\,} {\\frac {1}{2n_samples}} ||XW - Y||_Fro^2 + \\alpha \\rho ||W||_21 + \\frac{\\alpha(1-\\rho)}{2} ||W||_Fro^2\n",
    "$$\n",
    "\n",
    "在`MultiTaskElasticNet`类中的实现采用了坐标下降法求解参数\n",
    "在`MultiTaskElasticNetCV`中可以通过交叉验证来设置参数`alpha`(α)和`l1_ratio`(ρ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.7.最小角回归\n",
    "最小角回归(LARS)是对高维数据的回归算法，由Bradley Efron，Trevor Hastie，Lain Johnstone和Robert Tibshirani开发完成。LARS和逐步回归很像。在每一步，它都寻找与响应最有关联的预测。当有很多预测有相同的关联时，它并不会继续利用相同的预测，而是在这些预测中找出应该等角的方向。\n",
    "\n",
    "LARS的优点：\n",
    "- 当 p >> n，该算法数值运算上非常有效。（例如当维度的数目远超点的个数）\n",
    "- 它在计算上和前向选择一样快，和普通最小二乘法有相同的运算复杂度\n",
    "- 如果两个变量对响应几乎有相等的联系，则它们的系数应该有相似的增长率。因此这个算法和我们直觉上的判断一样，而且还更加稳定。\n",
    "- 它很容易修改并为其他估算器生成解，比如Lasso\n",
    "\n",
    "LARS的缺点：\n",
    "- 因为LARS是建立在循环拟合剩余变量上的，所以它对噪声非常敏感。\n",
    "\n",
    "LARS模型可以在`Lars`，或者它的底层实现`lars_path`或`lars_path_gram`中被使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.8.LARS Lasso\n",
    "`LassoLars`是一个LARS算法的lasso模型,不同于基于坐标下降法的实现，它可以得到一个精确解，也就是一个关于自身参数标准化的一个分段线性解\n",
    "\n",
    "<img src='../pics/reg-8.png' width='50%'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71715729, 0.        ])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LassoLars(alpha=.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lars算法提供了一个几乎无代价的沿着正则化参数的系数的完整路径，因此常利用函数`lars_path`或`lars_path_gram`来取回路径\n",
    "\n",
    "**数学表达式**  \n",
    "该算法和逐步回归非常相似，但是它没有在每一步包含变量，它估计的参数是根据与其他剩余变量的联系来增加。  \n",
    "在LARS的解中，没有给出一个向量的结果，而是给出了一条曲线，显示参数向量的L1范式的每个值的解。完全的参数路径存在`coef_path_`下。它的size是(n_samples, max_features+1)。其中第一列通常是全0列。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.9.正交匹配追踪法(OMP)\n",
    "`OrthogonalMatchingPursuit`(正交匹配追踪法)和`orthogonal_map`使用了OMP算法近似拟合了一个带限制的线性模型，该限制影响于模型的非0系数（例：L0范数）\n",
    "\n",
    "就像最小角回归一样，作为一个前向特征选择方法，正交匹配追踪法可以近似一个固定非0元素的最优向量解：\n",
    "$$ \n",
    "\\text{argmin} || y - X^2 \\gamma||_2^2 \\text {subject to} ||\\gamma||_0 ≤ \\leq n_{nonzero\\_coefs} \\\\\n",
    "$$\n",
    "\n",
    "正交匹配追踪法也可以针对一个特殊的误差而不是一个特殊的非零系数的个数。可以表示为：\n",
    "$$\n",
    "\\text{argmin} || \\gamma||_0 \\text{subject to} ||y - X \\gamma||_2^2 ≤ \\text{tol}   \n",
    "$$\n",
    "\n",
    "OMP是基于每一步的贪心算法，其每一步元素都是与当前残差高度相关的。它跟较为简单的匹配追踪(MP)很相似，但是比MP更好，在每一次迭代中，可以利用正交投影到之前选择的字典元素重新计算残差。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.10.贝叶斯回归\n",
    "贝叶斯回归可以用在预估阶段的参数正则化：正则化参数的选择不是通过人为的选择，而是通过手动调节数据值来实现。 \n",
    "\n",
    "上述过程可以通过引入无信息先验到模型中的超参数来完成。在`岭回归`中使用的$\\ell_2$正则项相当于在$\\omega$为高斯先验条件，且此先验的精确度为$\\lambda ^{-1}$时，求最大后验估计。在这里，我们没有手工调参数lambda，而是让他作为一个变量，通过数据中估计得到。\n",
    "\n",
    "为了得到一个全概率模型，输出$y$也被认为是关于$X\\omega$的高斯分布\n",
    "\n",
    "$$\n",
    "p(y|X,w,\\alpha) = \\mathcal{N}(y|X, w, \\alpha)\n",
    "$$\n",
    "\n",
    "Alpha在这里也是作为一个变量，通过数据中的估计得到。 \n",
    "\n",
    "贝叶斯回归有如下几个优点：\n",
    "- 它能根据已有的数据进行改变\n",
    "- 它能在估计过程中引入正则项\n",
    "\n",
    "贝叶斯回归有如下缺点：\n",
    "- 它的推断过程是非常耗时的\n",
    "\n",
    "**贝叶斯岭回归**  \n",
    "`BayesianRidge`利用概率模型估算了上述的回归问题，其先验参数$\\omega$是由以下球面高斯公式得出的：\n",
    "\n",
    "$$\n",
    "p(w|\\lambda) = \\mathcal{N}(w|0, \\lambda^{-1}{I_{p}})\n",
    "$$\n",
    "\n",
    "先验参数$\\alpha和\\lambda$一般是服从gamma分布，这个分布与高斯成共轭先验关系。得到的模型一般称为**贝叶斯岭回归**，并且这个与传统的`Rideg`非常相似。\n",
    "\n",
    "参数$\\omega, \\alpha和\\lambda$是在模型拟合的时候一起被估算出来的，其中参数$\\alpha和\\lambda$通过最大似然估计得到。  \n",
    "剩下的超参数$\\alpha_1, \\alpha_2, \\lambda_1以及\\lambda_2$是关于$\\alpha和\\lambda$的gamma分布的先验。它们通常被选择为**无信息先验**。默认$\\alpha_1=\\alpha_2=\\lambda_1=\\lambda_2=10^{-b}$\n",
    "\n",
    "<img src='../pics/reg-9.png' width='50%'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用贝叶斯岭回归来解决回归问题\n",
    "from sklearn import linear_model\n",
    "X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]\n",
    "Y = [0., 1., 2., 3.]\n",
    "reg = linear_model.BayesianRidge()\n",
    "reg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50000013])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在模型训练完成后，可以用来预测新值\n",
    "reg.predict([[1, 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49999993, 0.49999993])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#权值可以这样访问\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
