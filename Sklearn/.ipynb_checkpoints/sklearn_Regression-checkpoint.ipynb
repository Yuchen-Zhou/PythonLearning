{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn教程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果你还没有安装sklearn可以运行这行代码\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.监督学习Supervised Learning\n",
    "### 1.1 广义线性模型 Linear Model\n",
    "主要讲述一些用于回归的方法，其中目标值 y 是输入变量 x 的线性组合。 数学概念表示为：如果 $\\hat{y}$ 是预测值，那么有：\n",
    "$$\n",
    "\\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p\n",
    "$$\n",
    "\n",
    "在整个模块中，我们定义向量 w = (w_1,..., w_p) 作为 coef_ ，定义 w_0 作为 intercept_ \n",
    "\n",
    "\n",
    "#### 1.1.1 普通最小二乘法 Ordinary Least Squares\n",
    "`LinearRegression`拟合一个带有系数$w = (w_1, ..., w_p) $的线性模型，使数据集实际观测数据和预测数据（估计值）之间的残差平方和最小。其数学表达式为:\n",
    "$$\n",
    "\\underset{w}{min\\,} {|| X w - y||_2}^2\n",
    "$$\n",
    "\n",
    "<img src='../pics/sphx_glr_plot_ols_001.png' width='50%'>\n",
    "\n",
    "LinearRegression 会调用`fit`方法来拟合数组 X， y，并且将线性模型的系数 `w`存储在其成员变量 coef_ 中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，对于普通最小二乘的系数估计问题，其依赖于模型各项的相互独立性。当各项是相关的，且设计矩阵 `X` 的各列近似线性相关，那么，设计矩阵会趋向于奇异矩阵，这种特性导致最小二乘估计对于随机误差非常敏感，可能产生很大的方差。例如，在没有实验设计的情况下收集到的数据，这种多重共线性（multicollinearity）的情况可能真的会出现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**普通最小二乘法的复杂度**  \n",
    "该方法使用X的奇异值分解来计算最小二乘解。如果X是一个形状为`(x_samples, n_features)`的矩阵，设$n_{sample}\\geq_n{feature}$, 则该方法的复杂度为$O(n_{samples}n_{smaples}^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 岭回归\n",
    "Rige回归通过对系数的大小施加惩罚来解决普通最小二乘法的一些问题。岭系数最小化的是带惩罚项的残差平方和，\n",
    "$$\n",
    "\\underset{w}{min\\,} {{|| X w - y||_2}^2 + \\alpha {||w||_2}^2}\n",
    "$$\n",
    "\n",
    "其中，`α≥0`是控制系数收缩量的复杂性参数：α的值越大，收缩量越大，模型对共线性的鲁棒性也更强 \n",
    "<img src='../pics/reg-2.jpeg' wifth='50%'>\n",
    "\n",
    "与其他线性模型一样，Ridge用`fit()`方法完成拟合，并将模型系数`ω`存储在其`coef_`成员中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34545455 0.34545455]\n",
      "0.1363636363636364\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "reg = linear_model.Ridge(alpha= .5)\n",
    "x = np.array([[0, 0], [0, 0], [1, 1]])\n",
    "y = np.array([0, .1, 1])\n",
    "reg.fit(x, y)\n",
    "\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**岭回归的复杂度**\n",
    "与普通最小二乘法的复杂度是相同的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**设置正则化参数：广义交叉验证**  \n",
    "`RidgeCV`通过内置的关于的alpha参数的交叉验证来实现岭回归。该对象与GridSearchCV的使用方式相同，只是它默认为Generalized Cross-Validation（广义交叉验证GCV），这是一种有效的留一验证方法（LOO-CV）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "reg.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定cv属性的值将触发（通过GridSearchCV的）的交叉验证。例如，cv=10将出发10折的交叉验证，而不是广义交叉验证（GCV）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Lasso\n",
    "`Lasso`是拟合稀疏系统的线性模型。它在一些情况下是有用的，因为它倾向于使用较少参数值的情况，有效地减少给定解决方案所依赖变量的数量。因此，Lasso及其变体是压缩感知领域的基础。子啊一定条件下，它可以恢复一组非零权重的精确集。  \n",
    "在数学公式表达上，它由一个带有` \\ell_1 `先验的正则项的线性模型组成。其最小化的目标函数是：\n",
    "$$\n",
    "\\underset{w}{min\\,} { \\frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}\n",
    "$$\n",
    "lasso estimate解决了加上罚项$ \\alpha ||w||_1$的最小二乘法的最小化值，其中，α是一个常数，`||w||_1`是参数向量的`\\ell_1`-norm范数   \n",
    "\n",
    "`Lasso`类的实现使用了coordinate descent（坐标下降算法）来拟合系数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha= 0.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "reg.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**设置正则化参数**  \n",
    "`alpha`参数控制估计系数的稀疏度\n",
    "\n",
    "- 使用交叉验证  \n",
    "scikit-learn通过交叉验证来公开设置Lasso`alpha`参数的对象：`LassoCV`和`LassoLarsCV`。`LassoLarsCV`是基于下面将要提到的最小角回归算法。对于具有许多线性回归的高纬数据集，`LassoCV`最常见。然而，`LassoLarsCV`在寻找`alpha`参数值上更具有优势,而且如果样本数量比特征数量少得多时，通常`LassoLarsCV`比`LassoCV`要快。\n",
    "\n",
    "<img src='../pics/reg-3.png' width='50%'>\n",
    "\n",
    "<img src='../pics/reg-4.png' width='50%'>\n",
    "\n",
    "- 基于信息标准的模型选择\n",
    "有多种选择时，估计器`LassoLarsIC`建议使用Akaike information criterion(Akaike信息判断)（AIC）或Bayes Information criterion（贝叶斯信息判据）（BIC）。当使用f-fold交叉验证时，正则化路径只计算一次而不是k+1次，所以找到α的最优解值是一种计算上更经济的代替方法。然而，这种的判据需要对解决方案的自由度进行适当的估计，它会假设模型是正确的，对大样本（渐进结果）进行导出，即数据实际上是由该模型生成的。当问题严重受限（比样本更多的特征）时，它们也容易崩溃。\n",
    "<img src='../pics/reg-5.png' width='50%'>\n",
    "\n",
    "- 与SVM的正则化参数的比较\n",
    "`alpha`和SVM的正则化参数`C`之间的等式关系是`alpha = 1 / C`或`alpha = 1 / (n_samples * C)`，并依赖于估计器和模型优化的确切的目标函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
